# LÃ³gica Interna da LLM Chain - FunÃ§Ã£o get_llm_response()

Este documento descreve em detalhes a implementaÃ§Ã£o da funÃ§Ã£o `get_llm_response()` que gera respostas usando Qwen LLM via LangChain.

---

## ğŸ“‹ Assinatura da FunÃ§Ã£o

```python
async def get_llm_response(text: str, custom_context: str, db_context: str) -> str
```

**ParÃ¢metros:**
- `text`: Texto transcrito do Ã¡udio (input do usuÃ¡rio)
- `custom_context`: Contexto personalizado do usuÃ¡rio/KBF (PRIORIDADE 1)
- `db_context`: Contexto do RAG - base de conhecimento (PRIORIDADE 2)

**Retorno:**
- String contendo a resposta gerada pelo LLM

---

## ğŸ”„ PASSO 1: LÃ³gica de Prioridade de Contexto

### Regra de SeleÃ§Ã£o do `final_context`

A funÃ§Ã£o implementa uma lÃ³gica **if/elif/else** para determinar qual contexto serÃ¡ usado:

```python
if custom_context and custom_context.strip():
    # PRIORIDADE 1: Contexto do usuÃ¡rio/KBF
    final_context = custom_context.strip()
    context_source = "Contexto Personalizado do UsuÃ¡rio"

elif db_context and db_context.strip():
    # PRIORIDADE 2: Contexto do RAG (base de conhecimento)
    final_context = db_context.strip()
    context_source = "Base de Conhecimento Interna (RAG)"

else:
    # FALLBACK: Nenhum contexto disponÃ­vel
    final_context = "Nenhuma informaÃ§Ã£o adicional disponÃ­vel."
    context_source = "Sem Contexto EspecÃ­fico"
```

### Fluxograma da LÃ³gica

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Recebe: custom_context e db_context    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ custom_context     â”‚
        â”‚ nÃ£o estÃ¡ vazio?    â”‚
        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
             â”‚           â”‚
          SIMâ”‚           â”‚NÃƒO
             â”‚           â”‚
             â–¼           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚final_contextâ”‚  â”‚  db_context    â”‚
    â”‚= custom_    â”‚  â”‚ nÃ£o estÃ¡ vazio?â”‚
    â”‚  context    â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
    â”‚             â”‚       â”‚       â”‚
    â”‚PRIORIDADE 1 â”‚    SIMâ”‚       â”‚NÃƒO
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚       â”‚
                          â–¼       â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚final_contextâ”‚ â”‚  final_context  â”‚
                 â”‚= db_context â”‚ â”‚= "Nenhuma       â”‚
                 â”‚             â”‚ â”‚  informaÃ§Ã£o     â”‚
                 â”‚PRIORIDADE 2 â”‚ â”‚  adicional..."  â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                 â”‚
                                 â”‚  FALLBACK       â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Exemplos de ExecuÃ§Ã£o

| custom_context | db_context | final_context | context_source |
|---------------|------------|---------------|----------------|
| "VocÃª Ã© tÃ©cnico Python" | "Produto custa R$99" | "VocÃª Ã© tÃ©cnico Python" | Contexto Personalizado do UsuÃ¡rio |
| "" (vazio) | "Produto custa R$99" | "Produto custa R$99" | Base de Conhecimento Interna (RAG) |
| "" (vazio) | "" (vazio) | "Nenhuma informaÃ§Ã£o adicional disponÃ­vel." | Sem Contexto EspecÃ­fico |
| "   " (espaÃ§os) | "Produto custa R$99" | "Produto custa R$99" | Base de Conhecimento Interna (RAG) |

**Nota:** O mÃ©todo `.strip()` garante que strings com apenas espaÃ§os sejam tratadas como vazias.

---

## ğŸ—ï¸ PASSO 2: ConstruÃ§Ã£o do ChatPromptTemplate

O template Ã© composto por **3 PARTES DISTINTAS**:

### **PARTE 1: SystemPrompt - InstruÃ§Ã£o de Comportamento**

Define a **persona** e **comportamento** da IA.

```python
system_behavior_prompt = SystemMessagePromptTemplate.from_template(
    """VocÃª Ã© um assistente de voz inteligente da Empresa XPTO, especializado em fornecer respostas precisas e Ãºteis.

INSTRUÃ‡Ã•ES DE COMPORTAMENTO:
- Seja sempre educado, prestativo e profissional
- Responda de forma clara, concisa e objetiva
- Use linguagem natural e acessÃ­vel
- Mantenha um tom amigÃ¡vel mas profissional
- Se nÃ£o souber algo, seja honesto e nÃ£o invente informaÃ§Ãµes"""
)
```

**CaracterÃ­sticas:**
- âœ… Define quem Ã© a IA ("assistente da Empresa XPTO")
- âœ… Estabelece o tom de comunicaÃ§Ã£o (educado, profissional)
- âœ… Instrui sobre como responder (claro, conciso, honesto)
- âœ… Sem variÃ¡veis - conteÃºdo fixo

---

### **PARTE 2: SystemPrompt - Contexto do RAG/UsuÃ¡rio**

Insere o **contexto final** selecionado pela lÃ³gica de prioridade.

```python
system_context_prompt = SystemMessagePromptTemplate.from_template(
    """CONTEXTO ADICIONAL (Fonte: {context_source}):
{context}

COMO USAR O CONTEXTO:
- Se a pergunta do usuÃ¡rio estiver relacionada ao contexto acima, use essas informaÃ§Ãµes para fundamentar sua resposta
- Se a pergunta NÃƒO estiver relacionada ao contexto, responda com base em seu conhecimento geral
- Priorize sempre a precisÃ£o e relevÃ¢ncia da informaÃ§Ã£o"""
)
```

**VariÃ¡veis do Template:**
- `{context}` â†’ Preenchida com `final_context` (selecionado no PASSO 1)
- `{context_source}` â†’ Preenchida com `context_source` (indicador de origem)

**Exemplo de Preenchimento:**

**Entrada:**
```python
final_context = "Nosso produto xpto custa R$ 99,90 e tem garantia de 12 meses."
context_source = "Base de Conhecimento Interna (RAG)"
```

**Resultado apÃ³s preenchimento:**
```
CONTEXTO ADICIONAL (Fonte: Base de Conhecimento Interna (RAG)):
Nosso produto xpto custa R$ 99,90 e tem garantia de 12 meses.

COMO USAR O CONTEXTO:
- Se a pergunta do usuÃ¡rio estiver relacionada ao contexto acima, use essas informaÃ§Ãµes para fundamentar sua resposta
- Se a pergunta NÃƒO estiver relacionada ao contexto, responda com base em seu conhecimento geral
- Priorize sempre a precisÃ£o e relevÃ¢ncia da informaÃ§Ã£o
```

---

### **PARTE 3: HumanPrompt - Input do UsuÃ¡rio**

A pergunta/fala do usuÃ¡rio transcrita.

```python
human_prompt = HumanMessagePromptTemplate.from_template(
    "{user_input}"
)
```

**VariÃ¡vel do Template:**
- `{user_input}` â†’ Preenchida com `text` (transcriÃ§Ã£o do Ã¡udio)

**Exemplo de Preenchimento:**

**Entrada:**
```python
text = "Quanto custa o produto xpto?"
```

**Resultado apÃ³s preenchimento:**
```
Quanto custa o produto xpto?
```

---

### **COMBINAÃ‡ÃƒO: ChatPromptTemplate Completo**

```python
chat_prompt = ChatPromptTemplate.from_messages([
    system_behavior_prompt,   # PARTE 1: Persona
    system_context_prompt,    # PARTE 2: Contexto (variÃ¡vel)
    human_prompt              # PARTE 3: Pergunta do usuÃ¡rio
])
```

**Ordem das mensagens no chat:**
1. **System Message 1**: Comportamento da IA
2. **System Message 2**: Contexto adicional
3. **Human Message**: Pergunta do usuÃ¡rio

---

## ğŸ¤– PASSO 3: InicializaÃ§Ã£o do Modelo (ChatQwen)

```python
llm = get_qwen_llm()
```

A funÃ§Ã£o `get_qwen_llm()` retorna uma instÃ¢ncia de `ChatOpenAI` configurada para o modelo Qwen:

```python
ChatOpenAI(
    model=settings.qwen_model,           # Ex: "qwen-turbo"
    openai_api_key=settings.qwen_api_key,
    openai_api_base=settings.qwen_api_url,
    temperature=0.7,
    max_tokens=2000
)
```

**ParÃ¢metros:**
- `model`: Nome do modelo Qwen
- `temperature`: 0.7 (equilÃ­brio entre criatividade e determinismo)
- `max_tokens`: Limite de 2000 tokens na resposta

---

## ğŸ“¤ PASSO 4: CriaÃ§Ã£o do Parser de SaÃ­da

```python
output_parser = StrOutputParser()
```

O `StrOutputParser` converte a resposta bruta do LLM em uma **string limpa**.

**Sem parser:**
```python
AIMessage(content="O produto xpto custa R$ 99,90.", additional_kwargs={...})
```

**Com parser:**
```python
"O produto xpto custa R$ 99,90."
```

---

## â›“ï¸ PASSO 5: Montagem da Chain (LangChain Expression Language)

```python
chain = chat_prompt | llm | output_parser
```

**Estrutura da Chain:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ChatPrompt      â”‚ --> â”‚ ChatQwen â”‚ --> â”‚ StrOutput    â”‚
â”‚ Template        â”‚     â”‚   LLM    â”‚     â”‚   Parser     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“                       â†“                   â†“
  Formata msgs         Gera resposta        Extrai texto
```

**Operador `|` (pipe):**
- Conecta componentes em sequÃªncia
- SaÃ­da de um componente vira entrada do prÃ³ximo
- Suporta execuÃ§Ã£o assÃ­ncrona

---

## ğŸš€ PASSO 6: ExecuÃ§Ã£o da Chain

```python
response = await chain.ainvoke({
    "context": final_context,
    "context_source": context_source,
    "user_input": text
})
```

### Preenchimento das VariÃ¡veis

O dicionÃ¡rio passado para `ainvoke()` preenche as variÃ¡veis nos templates:

| VariÃ¡vel no Template | Valor Recebido | Origem |
|---------------------|----------------|--------|
| `{context}` | `final_context` | Selecionado pela lÃ³gica de prioridade (PASSO 1) |
| `{context_source}` | `context_source` | Indicador de origem do contexto |
| `{user_input}` | `text` | TranscriÃ§Ã£o do Ã¡udio |

### Exemplo Completo de ExecuÃ§Ã£o

**Entrada da FunÃ§Ã£o:**
```python
await get_llm_response(
    text="Quanto custa o produto xpto?",
    custom_context="",
    db_context="Nosso produto xpto custa R$ 99,90 com garantia de 12 meses."
)
```

**Passo 1 - SeleÃ§Ã£o de Contexto:**
```python
custom_context = ""  # vazio
db_context = "Nosso produto xpto custa R$ 99,90 com garantia de 12 meses."

# ELIF executado (PRIORIDADE 2)
final_context = "Nosso produto xpto custa R$ 99,90 com garantia de 12 meses."
context_source = "Base de Conhecimento Interna (RAG)"
```

**Passo 2 - Template Preenchido:**

```
[SYSTEM MESSAGE 1 - Comportamento]
VocÃª Ã© um assistente de voz inteligente da Empresa XPTO...
(instruÃ§Ãµes de comportamento)

[SYSTEM MESSAGE 2 - Contexto]
CONTEXTO ADICIONAL (Fonte: Base de Conhecimento Interna (RAG)):
Nosso produto xpto custa R$ 99,90 com garantia de 12 meses.

COMO USAR O CONTEXTO:
- Se a pergunta do usuÃ¡rio estiver relacionada ao contexto acima, use essas informaÃ§Ãµes...

[HUMAN MESSAGE - Pergunta]
Quanto custa o produto xpto?
```

**Passo 3-5 - Chain Executada:**
```
Template formatado â†’ Qwen LLM â†’ Parser
```

**Passo 6 - Resposta Final:**
```python
response = "O produto xpto custa R$ 99,90 e possui garantia de 12 meses, oferecendo qualidade e seguranÃ§a para sua compra."
```

---

## ğŸ“Š Diagrama de Fluxo Completo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ get_llm_response(text, custom_context, db_context)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ PASSO 1: LÃ³gica de         â”‚
        â”‚ Prioridade de Contexto     â”‚
        â”‚                            â”‚
        â”‚ IF custom_context?         â”‚
        â”‚   â†’ final_context = custom â”‚
        â”‚ ELIF db_context?           â”‚
        â”‚   â†’ final_context = db     â”‚
        â”‚ ELSE                       â”‚
        â”‚   â†’ final_context = fallbackâ”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ PASSO 2: Construir         â”‚
        â”‚ ChatPromptTemplate         â”‚
        â”‚                            â”‚
        â”‚ PARTE 1: Comportamento     â”‚
        â”‚ PARTE 2: {context}         â”‚
        â”‚ PARTE 3: {user_input}      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ PASSO 3: Inicializar       â”‚
        â”‚ ChatQwen (LLM)             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ PASSO 4: Criar             â”‚
        â”‚ StrOutputParser            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ PASSO 5: Montar Chain      â”‚
        â”‚ prompt | llm | parser      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ PASSO 6: Executar Chain    â”‚
        â”‚ ainvoke({                  â”‚
        â”‚   context: final_context,  â”‚â—„â”€â”€ Inserido aqui!
        â”‚   user_input: text         â”‚
        â”‚ })                         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ RETORNA: response (string) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Resumo da InserÃ§Ã£o do Contexto Final

### Como o `final_context` Ã© inserido na variÃ¡vel `{context}`

1. **SeleÃ§Ã£o (PASSO 1):**
   ```python
   final_context = custom_context OR db_context OR "Nenhuma informaÃ§Ã£o..."
   ```

2. **DefiniÃ§Ã£o no Template (PASSO 2):**
   ```python
   system_context_prompt = SystemMessagePromptTemplate.from_template(
       "CONTEXTO ADICIONAL: {context}"
   )
   ```

3. **Preenchimento na ExecuÃ§Ã£o (PASSO 6):**
   ```python
   chain.ainvoke({
       "context": final_context,  # â† AQUI acontece o preenchimento
       "user_input": text
   })
   ```

4. **Resultado:**
   - A string `{context}` no template Ã© **substituÃ­da** pelo valor de `final_context`
   - O LLM recebe a mensagem completa com o contexto jÃ¡ inserido
   - A resposta gerada jÃ¡ considera esse contexto

---

## ğŸ” Exemplo PrÃ¡tico: Fluxo Completo

### CenÃ¡rio 1: UsuÃ¡rio Define Contexto Personalizado

**Input:**
```python
get_llm_response(
    text="Como faÃ§o para instalar o sistema?",
    custom_context="VocÃª Ã© um tÃ©cnico especializado em Linux. Sempre forneÃ§a comandos bash.",
    db_context="O sistema Ã© compatÃ­vel com Windows e Mac."
)
```

**PASSO 1:** `final_context = "VocÃª Ã© um tÃ©cnico especializado em Linux..."`
**PASSO 6:** Template preenchido com contexto do usuÃ¡rio
**Resposta:** "Para instalar no Linux, execute: `sudo apt install sistema`..."

---

### CenÃ¡rio 2: Sem Contexto do UsuÃ¡rio, Usa RAG

**Input:**
```python
get_llm_response(
    text="Qual o horÃ¡rio de atendimento?",
    custom_context="",
    db_context="Atendimento de segunda a sexta, 9h Ã s 18h."
)
```

**PASSO 1:** `final_context = "Atendimento de segunda a sexta..."`
**PASSO 6:** Template preenchido com contexto do RAG
**Resposta:** "Nosso atendimento funciona de segunda a sexta-feira, das 9h Ã s 18h."

---

### CenÃ¡rio 3: Sem Nenhum Contexto

**Input:**
```python
get_llm_response(
    text="Quanto Ã© 2+2?",
    custom_context="",
    db_context=""
)
```

**PASSO 1:** `final_context = "Nenhuma informaÃ§Ã£o adicional disponÃ­vel."`
**PASSO 6:** Template preenchido com fallback
**Resposta:** "2 + 2 Ã© igual a 4."

---

## ğŸ“š Componentes do LangChain Utilizados

| Componente | Tipo | FunÃ§Ã£o |
|-----------|------|--------|
| `ChatPromptTemplate` | Prompt | Estrutura de mensagens para chat |
| `SystemMessagePromptTemplate` | Prompt | Define mensagens de sistema (comportamento + contexto) |
| `HumanMessagePromptTemplate` | Prompt | Define mensagem do usuÃ¡rio |
| `ChatOpenAI` | Model | Interface para Qwen LLM (compatÃ­vel OpenAI API) |
| `StrOutputParser` | Parser | Converte AIMessage em string |
| `chain.ainvoke()` | Execution | Executa chain assÃ­ncrona com inputs |

---

## âš™ï¸ Arquivo de ImplementaÃ§Ã£o

**LocalizaÃ§Ã£o:** `backend/services/qwen_service.py`

**FunÃ§Ã£o Principal:** `get_llm_response()` (linhas 23-148)

**FunÃ§Ã£o Wrapper:** `generate_response()` (linhas 154-171) - Para compatibilidade com cÃ³digo existente
